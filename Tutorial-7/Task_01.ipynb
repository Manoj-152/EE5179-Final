{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE3EcpRLYQ7E"
      },
      "source": [
        "# Implementing RNNs\n",
        "\n",
        "In this task, we will be building a machine learning model to classify sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "### Introduction\n",
        "\n",
        "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$. \n",
        "\n",
        "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
        "\n",
        "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$, (also known as a fully connected layer), to receive our predicted sentiment, $\\hat{y} = f(h_T)$.\n",
        "\n",
        "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
        "\n",
        "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpzjcO5hYQ7J"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "id": "p1gxbKdSYjXU",
        "outputId": "b55ffaf7-1951-4fee-80b8-830dc616ff82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmZyFrKCYQ7V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Anzeep8YQ7X"
      },
      "source": [
        "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP). \n",
        "\n",
        "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80KT1vLvYQ7X",
        "outputId": "0a10d5e6-1604-4d14-b7e5-9cfd76f5595e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchtext.legacy import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo4OyBFgYQ7Y"
      },
      "source": [
        "We can see how many examples are in each split by checking their length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAHqJxYHYQ7Z",
        "outputId": "5c98cb9a-130f-4c2b-f2e1-bea5c5d0d9fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULYFH0I5YQ7b"
      },
      "source": [
        "We can also check an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo3wFzsdYQ7b",
        "outputId": "e6f52c24-a6eb-46f3-b551-7fcf2a389330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['In', 'New', 'York', ',', 'Andy', 'Hanson', '(', 'Philip', 'Seymour', 'Hoffman', ')', 'is', 'an', 'addicted', 'executive', 'of', 'a', 'real', 'estate', 'office', 'that', 'has', 'embezzled', 'a', 'large', 'amount', 'for', 'his', 'addiction', 'and', 'expensive', 'way', 'of', 'life', 'with', 'his', 'wife', 'Gina', '(', 'Marisa', 'Tomei', ')', '.', 'When', 'an', 'audit', 'is', 'scheduled', 'in', 'his', 'department', ',', 'he', 'becomes', 'desperate', 'for', 'money', '.', 'His', 'baby', 'brother', 'Hank', 'Hanson', '(', 'Ethan', 'Hawke', ')', 'is', 'a', 'complete', 'loser', 'that', 'owes', 'three', 'months', 'of', 'child', 'support', 'to', 'his', 'daughter', ',', 'and', 'is', 'having', 'a', 'love', 'affair', 'with', 'Gina', 'every', 'Thursday', 'afternoon', '.', 'Andy', 'plots', 'a', 'heist', 'of', 'the', 'jewelry', 'of', 'their', 'parent', 'in', 'a', 'Saturday', 'morning', 'without', 'the', 'use', 'of', 'guns', ',', 'expecting', 'to', 'find', 'an', 'old', 'employee', 'working', 'and', 'without', 'financial', 'damage', 'to', 'his', 'parents', ',', 'since', 'the', 'insurance', 'company', 'would', 'reimburse', 'the', 'loss', '.', 'On', 'Monday', 'morning', ',', 'we', 'would', 'raise', 'the', 'necessary', 'money', 'he', 'needs', 'to', 'cover', 'his', 'embezzlement', '.', 'He', 'invites', 'Hank', 'to', 'participate', ',', 'since', 'he', 'is', 'very', 'well', 'known', 'in', 'the', 'mall', 'where', 'the', 'jewelry', 'is', 'located', 'and', 'could', 'be', 'recognized', '.', 'However', ',', 'Hank', 'yellows', 'and', 'invites', 'the', 'thief', 'Bobby', 'Lasorda', '(', 'Brian', 'F.', \"O'Byrne\", ')', 'to', 'steal', 'the', 'store', ',', 'but', 'things', 'go', 'wrong', 'when', 'their', 'mother', 'Nanette', '(', 'Rosemary', 'Harris', ')', 'comes', 'to', 'work', 'as', 'the', 'substitute', 'for', 'the', 'clerk', 'and', 'Bobby', 'brings', 'a', 'hidden', 'gun', '.', 'Nanette', 'reacts', 'and', 'kills', 'Bobby', 'but', 'she', 'is', 'also', 'lethally', 'shot', '.', 'After', 'the', 'death', 'of', 'Nanette', ',', 'their', 'father', 'Charles', 'Hanson', '(', 'Albert', 'Finney', ')', 'decides', 'to', 'investigate', 'the', 'robbery', 'with', 'tragic', 'consequences.<br', '/><br', '/>\"Before', 'the', 'Devil', 'Knows', 'You', \"'re\", 'Dead', '\"', 'is', 'a', 'comedy', 'of', 'errors', ',', 'disclosing', 'a', 'good', 'story', '.', 'The', 'originality', 'and', 'the', 'difference', 'are', 'in', 'the', 'screenplay', ',', 'with', 'a', 'non', '-', 'linear', 'narrative', 'à', 'la', '\"', 'Pulp', 'Fiction', '\"', '.', 'The', 'eighty', '-', 'three', 'year', '-', 'old', 'Sidney', 'Lumet', 'has', 'another', 'great', 'work', 'and', 'it', 'is', 'impressive', 'the', 'longevity', 'of', 'this', 'director', '.', 'Philip', 'Seymour', 'Hoffman', 'is', 'awesome', 'in', 'the', 'role', 'of', 'a', 'dysfunctional', 'man', 'with', 'traumatic', 'relationship', 'with', 'his', 'father', 'that', 'feels', 'the', 'world', 'falling', 'apart', 'mostly', 'because', 'of', 'his', 'insecure', 'and', 'clumsy', 'brother', '.', 'Marisa', 'Tomei', 'is', 'still', 'impressively', 'gorgeous', 'and', 'sexy', ',', 'showing', 'a', 'magnificent', 'body', '.', 'The', 'violent', 'conclusion', 'shows', 'that', 'the', 'world', 'is', 'indeed', 'an', 'evil', 'place', '.', 'My', 'vote', 'is', 'seven.<br', '/><br', '/>Title', '(', 'Brazil', '):', '\"', 'Antes', 'Que', 'o', 'Diabo', 'Saiba', 'Que', 'Você', 'Está', 'Morto', '\"', '(', '\"', 'Before', 'the', 'Devil', 'Knows', 'You', \"'re\", 'Dead', '\"', ')'], 'label': 'pos'}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9InKM6hOYQ7d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji30mExiYQ7e",
        "outputId": "616323b4-82c4-436a-c758-f4cfe62c0eab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mozGw3ysYQ7e"
      },
      "source": [
        "Next, we have to build a _vocabulary_. This is simply a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment5.png?raw=1)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one). \n",
        "\n",
        "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\".\n",
        "\n",
        "The following builds the vocabulary, only keeping the most common `max_size` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRokjRf4YQ7f"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eU7T-PPYQ7f"
      },
      "source": [
        "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBp-C_PjYQ7f",
        "outputId": "adaa04ab-8bbd-4939-ab35-8dfe2a9b4663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D6HGAkKYQ7g"
      },
      "source": [
        "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
        "\n",
        "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment6.png?raw=1)\n",
        "\n",
        "We can also view the most common words in the vocabulary and their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-M_w0JqYQ7g",
        "outputId": "9fc3accf-0355-4513-bbf5-e37bbacd8f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 203099), (',', 194338), ('.', 166667), ('a', 110130), ('and', 109629), ('of', 101120), ('to', 93927), ('is', 76747), ('in', 61591), ('I', 54477), ('it', 53687), ('that', 49236), ('\"', 44725), (\"'s\", 43553), ('this', 42409), ('-', 37586), ('/><br', 35867), ('was', 34888), ('as', 30564), ('with', 29962)]\n"
          ]
        }
      ],
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbdycc-XYQ7g"
      },
      "source": [
        "We can also see the vocabulary directly using either the `stoi` (**s**tring **to** **i**nt) or `itos` (**i**nt **to**  **s**tring) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ8ycUU8YQ7h",
        "outputId": "0e98da8e-d5c3-4244-c334-021a168ebc44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
          ]
        }
      ],
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_3yQln7YQ7h"
      },
      "source": [
        "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zoRETsYYQ7i",
        "outputId": "4ba4757c-2684-40ba-82e5-a7b4163ab7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ]
        }
      ],
      "source": [
        "print(LABEL.vocab.stoi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHGSWja-YQ7i"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`, we then pass this device to the iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxx54-k8YQ7i"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REam55tYQ7i"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate. \n",
        "\n",
        "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
        "\n",
        "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
        "\n",
        "The `forward` method is called when we feed examples into our model.\n",
        "\n",
        "Each batch, `text`, is a tensor of size _**[sentence length, batch size]**_. That is a batch of sentences, each having each word converted into a one-hot vector. \n",
        "\n",
        "The RNN returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of size _**[1, batch size, hidden dim]**_. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this using the `assert` statement. Note the `squeeze` method, which is used to remove a dimension of size 1. \n",
        "\n",
        "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuEHlHaxYQ7j"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEVyKe7DYQ7k"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGOAKoAZYQ7k"
      },
      "source": [
        "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgWx7indYQ7k",
        "outputId": "bfdcac2e-6d44-4107-fb83-a2e7fad9529e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYOKyX1WYQ7k"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACY--SrXYQ7l"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkzhZ6K_YQ7m"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UgV_ONRjf86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXxdTxqmYQ7n"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in tqdm(iterator):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePjSIylqYQ7n"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in tqdm(iterator):\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFMpeJclYQ7o"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV1USNLhYQ7o",
        "outputId": "97e34643-64a9-435d-a2a9-2fe6a5fdeb93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [00:15<00:00, 17.16it/s]\n",
            "100%|██████████| 118/118 [00:01<00:00, 98.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 17s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.94%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [00:15<00:00, 18.06it/s]\n",
            "100%|██████████| 118/118 [00:01<00:00, 99.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.65%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [00:15<00:00, 17.82it/s]\n",
            "100%|██████████| 118/118 [00:01<00:00, 98.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.25%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 49.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [00:15<00:00, 18.22it/s]\n",
            "100%|██████████| 118/118 [00:01<00:00, 98.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.78%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 274/274 [00:15<00:00, 17.85it/s]\n",
            "100%|██████████| 118/118 [00:01<00:00, 98.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.45%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut7-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBrD9StUYQ7p",
        "outputId": "567d52c3-445e-4e26-f2b9-e203fe342747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 97.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.707 | Test Acc: 48.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut7-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQEQ0RT5YQ7p"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "In the next task, the improvements we will make are:\n",
        "- packed padded sequences\n",
        "- pre-trained word embeddings\n",
        "- different RNN architecture\n",
        "- bidirectional RNN\n",
        "- multi-layer RNN\n",
        "- regularization\n",
        "- a different optimizer\n",
        "\n",
        "This will allow us to achieve ~84% accuracy."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}